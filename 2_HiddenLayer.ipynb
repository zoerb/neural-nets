{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afa7990c",
   "metadata": {},
   "source": [
    "Multi-layer perceptron with two input neurons, a hidden layer with 3 neurons, and a single output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873152b5-281d-4f49-b280-ca4c67a3b18b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights connecting input layer to hidden layer\n",
    "W0 = np.array([[0.61275508, 0.98539237, 0.27266151],\n",
    "               [-0.47399039, 0.66266751, -0.72721264]])\n",
    "\n",
    "# Weights connecting hidden layer to output layer\n",
    "W1 = np.array([[0.96787958],\n",
    "               [-0.80183257],\n",
    "               [0.94478099]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8298eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "  x1 = np.matmul(x, W0) # Hidden layer values\n",
    "  x2 = np.matmul(x1, W1) # Output values\n",
    "  return x1, x2\n",
    "\n",
    "def loss(out):\n",
    "  return abs(expected - out).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, out = forward(input)\n",
    "l = loss(out)\n",
    "print(l)\n",
    "\n",
    "plot(forward(test)[1])\n",
    "#np.column_stack([x2, expected])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "deba59ce",
   "metadata": {},
   "source": [
    "Chain rule for backprop:\n",
    "\n",
    "dL/dW1 = dL/dOut * dOut/dW1 = dL/dOut * x1\n",
    "\n",
    "dL/dH = dL/dOut * dOut/dH\n",
    "\n",
    "dL/dW0 = dL/dH * dH/dW0 = dL/dH * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc524bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample (the rows), how does a change to `out` change the loss\n",
    "# Positive values: as `out` increases, loss increases\n",
    "# Negative values: as `out` increases, loss decreases\n",
    "dLdOut = -np.sign(expected - out)\n",
    "dLdOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6af8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample, how does a change to each of the W1 weights change the loss\n",
    "dLdW1PerSample = elemOuter(h, dLdOut)\n",
    "# Sum over samples to get overall W1 derivatives across all samples\n",
    "dLdW1 = dLdW1PerSample.sum(0)\n",
    "dLdW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample, how does a change to each hidden node change the loss\n",
    "dLdH = np.matmul(dLdOut, W1.T)\n",
    "dLdH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does a change to each of the W0 weights change the loss\n",
    "dLdW0PerSample = elemOuter(input, dLdH)\n",
    "dLdW0 = dLdW0PerSample.sum(0)\n",
    "dLdW0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "\n",
    "# Multiply learning rate by the derivatives to get the weight deltas\n",
    "W0 -= lr * dLdW0\n",
    "W1 -= lr * dLdW1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
