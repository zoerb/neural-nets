{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873152b5-281d-4f49-b280-ca4c67a3b18b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 5, 5, 1]\n",
    "\n",
    "# Weights\n",
    "W = [np.random.rand(m, n) * 2 - 1 for m, n in zip(layers, layers[1:])]\n",
    "\n",
    "expected = expected.reshape(9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W\n",
    "xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8298eb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.30586346029024"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (45,25) (9,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(expected \u001b[39m-\u001b[39m xi[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39msum()\n\u001b[1;32m     45\u001b[0m loss\n\u001b[0;32m---> 47\u001b[0m backprop(xi)\n",
      "Cell \u001b[0;32mIn[96], line 20\u001b[0m, in \u001b[0;36mbackprop\u001b[0;34m(xi)\u001b[0m\n\u001b[1;32m     16\u001b[0m dLdX\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mouter(dLdX[i], W[\u001b[39m-\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)]))\n\u001b[1;32m     18\u001b[0m \u001b[39m# For each sample, in which direction do each of the layer's weights need\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# to be nudged\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m dLdWPerSample \u001b[39m=\u001b[39m dLdX[i] \u001b[39m*\u001b[39;49m xi[\u001b[39m-\u001b[39;49m(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)]\n\u001b[1;32m     22\u001b[0m \u001b[39m# Sum over samples to get overall update to W1\u001b[39;00m\n\u001b[1;32m     23\u001b[0m dLdW\u001b[39m.\u001b[39mappend(dLdWPerSample\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (45,25) (9,5) "
     ]
    }
   ],
   "source": [
    "def run(x):\n",
    "  xi = [x]\n",
    "\n",
    "  for i in range(len(layers) - 1):\n",
    "    xi.append(np.matmul(xi[i], W[i]))\n",
    "\n",
    "  return xi\n",
    "  \n",
    "dLdX = []\n",
    "dLdW = []\n",
    "\n",
    "def backprop(xi):\n",
    "  dLdX = [-np.sign(expected - xi[-1])]\n",
    "\n",
    "  for i in range(len(layers)):\n",
    "    dLdX.append(np.outer(dLdX[i], W[-(i+1)]))\n",
    "\n",
    "    # For each sample, in which direction do each of the layer's weights need\n",
    "    # to be nudged\n",
    "    dLdWPerSample = dLdX[i] * xi[-(i+1)]\n",
    "\n",
    "    # Sum over samples to get overall update to W1\n",
    "    dLdW.append(dLdWPerSample.sum(axis=0))\n",
    "\n",
    "    f = lambda i, j: np.outer(i, j)\n",
    "    vecF = np.vectorize(f, signature='(m),(n)->(m,n)')\n",
    "\n",
    "    #dLdW0PerSample = vecF(input, dLdX1)\n",
    "    #dLdW0 = dLdW0PerSample.sum(axis=0)\n",
    "    #dLdW0\n",
    "  \n",
    "  dLdX.reverse()\n",
    "  dLdW.reverse()\n",
    "\n",
    "def updateWeights():\n",
    "  e = 1e-2\n",
    "  \n",
    "  for i in range(len(layers) - 1):\n",
    "    W[i] -= e * dLdW[i]\n",
    "\n",
    "\n",
    "\n",
    "xi = run(input)\n",
    "loss = abs(expected - xi[-1]).sum()\n",
    "loss\n",
    "\n",
    "backprop(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(test[:, 0], test[:, 1], c=run(test)[0].squeeze(), cmap=cmap)\n",
    "#plt.imshow(run(test)[0].reshape(256, 256), cmap=cmap, norm=CenteredNorm(0, 0.1))\n",
    "plt.imshow(run(test)[-1].reshape(256, 256), cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc524bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample (the rows), in which direction does x2 need to be nudged\n",
    "dLdX2 = -np.sign(expected - x2)\n",
    "dLdX2\n",
    "# For each sample, in which direction do each of the W1 weights need to be nudged\n",
    "dLdW1PerSample = dLdX2 * x1\n",
    "# Sum over samples to get overall update to W1\n",
    "dLdW1 = dLdW1PerSample.sum(axis=0)\n",
    "dLdW1\n",
    "\n",
    "dLdX1 = np.outer(dLdX2, W1)\n",
    "dLdX1\n",
    "\n",
    "f = lambda i, j: np.outer(i, j)\n",
    "vecF = np.vectorize(f, signature='(m),(n)->(m,n)')\n",
    "\n",
    "dLdW0PerSample = vecF(input, dLdX1)\n",
    "dLdW0 = dLdW0PerSample.sum(axis=0)\n",
    "dLdW0\n",
    "\n",
    "e = 1e-2\n",
    "W0 -= e * dLdW0\n",
    "W1 -= e * dLdW1.reshape(3, 1)\n",
    "\n",
    "W0\n",
    "W1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
